{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/sagar/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from os import walk\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dir = \"/media/sagar/D/ctranz/newDataset/train\"\n",
    "test_dataset_dir = \"/media/sagar/D/ctranz/newDataset/validate\"\n",
    "img_size = 224\n",
    "img_channels = 3\n",
    "num_classes = 4\n",
    "data_dict = {'data':None, 'labels':[], 'image_path':[]}\n",
    "#Load the ResNet50 model\n",
    "#resnet_model = resnet50.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 277 images belonging to 4 classes.\n",
      "Found 67 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 60\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                width_shift_range=0.2,\n",
    "                                height_shift_range=0.2,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode='nearest')\n",
    "\n",
    "#img = load_img('/media/sagar/D/ctranz/newDataset/test/SUV00.jpg')  # this is a PIL image\n",
    "#x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "#x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "#print(x.shape)\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "        \n",
    "        \n",
    "        \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    train_dataset_dir,\n",
    "                                    target_size=(img_size,img_size),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                    test_dataset_dir,\n",
    "                                    target_size=(img_size,img_size),\n",
    "                                    batch_size=batch_size,\n",
    "                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False,input_shape=input_shape)\n",
    "#base_model.summary()\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(base_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from resnet50. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "#for i, layer in enumerate(base_model.layers):\n",
    " #  print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from resnet50. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "\n",
    "nclass =4\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "#x = base_model.output\n",
    "#x = Flatten()(x)\n",
    "#predictions = Dense(3, activation='softmax', name='predictions')(x)\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "#predictions = Dense(5,activation='softmax')(x)\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "#predictions = Dense(4, activation='softmax')(x)\n",
    "add_model = Sequential()\n",
    "base_model.trainable = False\n",
    "add_model.add(base_model)\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "#add_model.add(Flatten())\n",
    "#add_model.add(Dropout(0.8))\n",
    "#add_model.add(Dense(4000, activation='relu'))\n",
    "#add_model.add(Dropout(0.5))\n",
    "add_model.add(Dense(1024, activation='relu'))\n",
    "add_model.add(Dense(512, activation='relu'))\n",
    "add_model.add(Dropout(0.5))\n",
    "add_model.add(Dense(nclass,activation='softmax'))\n",
    "model = add_model\n",
    "\n",
    "#model = Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "#for i, layer in enumerate(model.layers):\n",
    "#   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "#for layer in base_model.layers:\n",
    "#    layer.trainable = False\n",
    "from keras.optimizers import Adam\n",
    "#Adam = optimizers.Adam(lr=0.001)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 35s - loss: 1.6981 - acc: 0.3894 - val_loss: 1.5931 - val_acc: 0.2836\n",
      "Epoch 2/20\n",
      " - 34s - loss: 1.3928 - acc: 0.4419 - val_loss: 1.1311 - val_acc: 0.5075\n",
      "Epoch 3/20\n",
      " - 34s - loss: 1.1741 - acc: 0.5108 - val_loss: 1.1031 - val_acc: 0.5821\n",
      "Epoch 4/20\n",
      " - 33s - loss: 1.1197 - acc: 0.4873 - val_loss: 0.9852 - val_acc: 0.5373\n",
      "Epoch 5/20\n",
      " - 31s - loss: 0.9254 - acc: 0.6210 - val_loss: 0.9746 - val_acc: 0.4776\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, steps_per_epoch=None, epochs=20, verbose=2, callbacks=None, validation_data=validation_generator, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=True, shuffle=True, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from resnet50. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "#for i, layer in enumerate(model.layers):\n",
    "#   print(i, layer.name)\n",
    "#model.fit_generator(train_generator, steps_per_epoch=None, epochs=50, verbose=2, callbacks=None, validation_data=test_generator, validation_steps=None, class_weight=None, max_queue_size=10, workers=4, use_multiprocessing=True, shuffle=True, initial_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:150]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[150:]:\n",
    "    layer.trainable = True\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=None, epochs=20, verbose=2, callbacks=None, validation_data=validation_generator, validation_steps=None, class_weight=None, max_queue_size=10, workers=4, use_multiprocessing=True, shuffle=True, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit_generator(train_generator, steps_per_epoch=None, epochs=30, verbose=2, callbacks=None, validation_data=validation_generator, validation_steps=None, class_weight=None, max_queue_size=10, workers=4, use_multiprocessing=True, shuffle=True, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "label_map = (train_generator.class_indices)\n",
    "print(label_map)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/media/sagar/D/ctranz/newDataset/test/SUV79.png\"\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "print(x.shape)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "preds = model.predict(x)\n",
    "y_classes = preds.argmax(axis=-1)\n",
    "print('class:', y_classes)\n",
    "print('Predicted:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
